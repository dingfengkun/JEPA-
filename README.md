
# JEPA: Joint Embedding Predictive Architecture (PyTorch)

This repository implements a JEPAâ€‘style selfâ€‘supervised learner: it predicts **latent representations** of masked portions of an input (image or video) from the **context embeddings**, avoiding costly pixel reconstruction and learning highâ€‘level semantics.

---

## ğŸš€ Key Features

- **Latentâ€‘space prediction**: Predict only embedding vectors of masked patches, not raw pixels.  
- **Asymmetric dual encoders**:  
  - **Context encoder** (trainable)  
  - **Target encoder** (EMA of context) for stable â€œteacherâ€ targets.  
- **Blockâ€‘wise masking**: Randomly mask multiple patches; predictor reconstructs them from context.  
- **Lightweight predictor head**: Small MLP or transformer head focuses training on encoder quality.  
- **Flexible backbone**: Swap between CNNs, ResNets, ViTs, or even energy transformers.  
- **Optional extra losses**: Huber/MSE on embeddings, cycleâ€‘consistency, VICReg invariance/covariance.

---

## ğŸ“ Repository Structure

```

DL25SP/
â”œâ”€â”€ configs/                 # YAML files for experiments
â”œâ”€â”€ dataset.py               # data loading & blockâ€‘masking utilities
â”œâ”€â”€ jepa\_model.py            # context encoder, target encoder (EMA), predictor
â”œâ”€â”€ losses.py                # embedding prediction losses (MSE, Huberâ€¦)
â”œâ”€â”€ train\_jepa.py            # training loop, EMA updates, logging
â”œâ”€â”€ evaluator.py             # linear probe / KNN evaluation on frozen encoder
â”œâ”€â”€ utils.py                 # misc helpers (EMA, metrics, checkpoints)
â””â”€â”€ README.md                # this documentation

````

---

## âš™ï¸ Installation

```bash
git clone https://github.com/dingfengkun/JEPA.git
cd JEPA
pip install -r requirements.txt
````

Dependencies include: `torch`, `torchvision`, `pyyaml`, `einops`, `tqdm`.

---

## ğŸ‹ï¸â€â™‚ï¸ Training

Edit or select a config in `configs/`. A minimal example:

```yaml
# configs/imagenet_vit.yaml
backbone: vit_base_patch16_224
mask_ratio: 0.5
batch_size: 256
learning_rate: 1e-4
epochs: 100
ema_decay: 0.99
loss_type: huber
```

Launch training:

```bash
python train_jepa.py --config configs/imagenet_vit.yaml
```

**What happens inside**:

1. Sample a batch, randomly mask patches per image.
2. Encode **context** patches with the trainable encoder.
3. Encode **target** (small patches) with the EMA encoder.
4. Predictor takes context embeddings (+ positional info) â†’ predicts target embeddings.
5. Compute Huber/MSE loss between predicted vs. EMAâ€‘encoded target.
6. Backprop on context encoder + predictor; update EMA encoder.

---

## ğŸ“Š Evaluation

After pretraining, freeze **context encoder** and run a **linear probe** or **KNN** on downstream classification:

```bash
python evaluator.py --model_path checkpoints/best.pth \
                    --dataset cifar10 \
                    --eval_mode linear_probe
```

---

## ğŸ› ï¸ Tips for lowering loss further

* **Backbone**: upgrade to a larger ViT or ResNet gives richer embeddings.
* **Mask scheduling**: gradually increase the mask ratio over epochs.
* **Loss**: try Huber over MSE if you see large outliers.
* **Additional regularizers**: add cycleâ€‘consistency or VICReg terms (see community forks).
* **Optimizer**: AdamW with cosine scheduler often yields smoother convergence.

---

## ğŸ”– Citation

If you use this code, please cite:

```bibtex
@article{assran2023self,
  title={Selfâ€‘Supervised Learning from Images with a Jointâ€‘Embedding Predictive Architecture},
  author={Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  journal={arXiv preprint arXiv:2301.08243},
  year={2023}
}
```

---

That README captures all the **core mechanisms** (latent prediction, EMA, masking) that drive down loss, and gives users concrete guidance on structure, training, evaluation, and further tweaks. Let me know if youâ€™d like to include example training curves or code snippets inside!
::contentReference[oaicite:5]{index=5}
```

[1]: https://github.com/facebookresearch/ijepa?utm_source=chatgpt.com "facebookresearch/ijepa: Official codebase for I-JEPA, the ... - GitHub"
[2]: https://github.com/gaasher/I-JEPA?utm_source=chatgpt.com "gaasher/I-JEPA - GitHub"
[3]: https://github.com/LumenPallidium/jepa?utm_source=chatgpt.com "Experiments in Joint Embedding Predictive Architectures (JEPAs)."
